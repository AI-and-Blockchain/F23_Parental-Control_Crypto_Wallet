{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from fedlab.contrib.algorithm.basic_client import SGDSerialClientTrainer\n",
    "from fedlab.contrib.algorithm.basic_server import SyncServerHandler\n",
    "from fedlab.core.standalone import StandalonePipeline\n",
    "from fedlab.utils.functional import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible features to implement:\n",
    "\n",
    "- Transaction History\n",
    "    - Amt, Timestamps -> Frequency\n",
    "- Current Balance\n",
    "- User interaction\n",
    "- Geolocation Data\n",
    "- Time patterns\n",
    "- How often users redeem rewards\n",
    "- Wallet features used\n",
    "- Financial Goals\n",
    "\n",
    "-Some attributes are categorical like user interaction. We might have to do some sort of engagement leveling for that.\n",
    "- Wallet features can indicate what kind of resources a user might desire:\n",
    "    - If they like to check their balance more than making transactions, it might be a sign that a user is considering making a purchase but is nervous about consequences regarding it. This could be \"scenario 1\" and can be encoded as a one hot vector like [1, 0, 0, ..., 0]\n",
    "\n",
    "### NOTE\n",
    "\n",
    "- Most features are tentative and may not be implemented. It is unclear as to what kind of data we will have access to at the current moment and whether or not hte collection of this data is feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype read data function.\n",
    "df = pd.read_csv(\"user-data.csv\", delimiter=\";\")\n",
    "\n",
    "# Data matrix\n",
    "D = df.to_numpy()\n",
    "\n",
    "# Presumably, (# of points, 8 + # of wallet features)\n",
    "print(D.shape)\n",
    "\n",
    "\n",
    "# It is likely that we will have to make our own assessments on each dataset.\n",
    "X, Y = D[:, :-1], D[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction history, columns 0 and 1\n",
    "\n",
    "def spendings(Amt, Timestamps, Balances):\n",
    "    # I don't know if what we'll be storing are datetime objects so I'm\n",
    "    # just going to be careful and say no.\n",
    "    Timestamps = pd.to_datetime(Timestamps)\n",
    "\n",
    "    # Calculate difference in minutes\n",
    "    time_diffs = np.diff(Timestamps).astype('timedelta64[m]').astype(int)\n",
    "\n",
    "    # Normalization.\n",
    "    norm_time_diffs = 1 / (time_diffs + 1)\n",
    "    norm_balances = (Balances - Amt) / Balances\n",
    "\n",
    "    # Combine the scores: lower values for both indicate potentially reckless spending\n",
    "    return norm_balances * norm_time_diffs\n",
    "\n",
    "def normalize(v):\n",
    "    return 2 * ( (v - np.amin(v)) / (np.amax(v) - np.amin(v)) ) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_matrix(X):\n",
    "    # Spendings dictates how much of their balance is spent per transaction\n",
    "    # within a certain time interval.\n",
    "    a1 = spendings(X[:, 0], X[:, 1], X[:, 2])\n",
    "\n",
    "    # Normalized number of queries (user interaction)\n",
    "    a2 = normalize(v=X[:, 3])\n",
    "\n",
    "    # TODO: Other features...\n",
    "\n",
    "    return np.column_stack((a1, a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is code heavily based on Zaki's implementation of a Simple Neural Network.\n",
    "\n",
    "# def relu(z):\n",
    "#     \"\"\"Apply the ReLU (Rectified Linear Unit) function.\"\"\"\n",
    "#     return np.maximum(0, z)\n",
    "\n",
    "# def relu_derivative(z):\n",
    "#     \"\"\"Compute the derivative of the ReLU function.\"\"\"\n",
    "#     return np.where(z > 0, 1, 0)\n",
    "\n",
    "# def feed_forward(x, network):\n",
    "#     \"\"\"Perform a feedforward pass through the neural network.\"\"\"\n",
    "#     activations = [x]\n",
    "#     input_to_layer = x\n",
    "\n",
    "#     for layer in network:\n",
    "#         z = layer['b'] + np.dot(layer['W'].T, input_to_layer)\n",
    "#         input_to_layer = relu(z)\n",
    "#         activations.append(input_to_layer)\n",
    "\n",
    "#     activations[-1] = softmax(activations[-1])\n",
    "#     return activations\n",
    "\n",
    "# def initialize_network(input_size, hidden_layer_sizes, output_size, scale):\n",
    "#     \"\"\"Initialize a deep multilayer perceptron with random weights and biases.\"\"\"\n",
    "#     layer_sizes = [input_size] + hidden_layer_sizes + [output_size]\n",
    "#     network = []\n",
    "\n",
    "#     for i in range(len(layer_sizes) - 1):\n",
    "#         layer = {\n",
    "#             'b': np.random.rand(layer_sizes[i + 1]) * scale,\n",
    "#             'W': np.random.rand(layer_sizes[i], layer_sizes[i + 1]) * scale\n",
    "#         }\n",
    "#         network.append(layer)\n",
    "\n",
    "#     return network\n",
    "\n",
    "# def deep_mlp_training(data, output_size, max_iter, learning_rate, hidden_layer_sizes, scale):\n",
    "#     \"\"\"Train a deep multilayer perceptron on the given dataset.\"\"\"\n",
    "#     num_samples, num_features = data.shape\n",
    "#     input_size = num_features - 1  # Last column is assumed to be the label\n",
    "#     network = initialize_network(input_size, hidden_layer_sizes, output_size, scale)\n",
    "\n",
    "#     for j in range(max_iter):\n",
    "#         indices = np.arange(num_samples)\n",
    "#         np.random.shuffle(indices)\n",
    "\n",
    "#         for i in indices:\n",
    "#             x_i = data[i, :-1]\n",
    "#             y_i = np.zeros(output_size)\n",
    "#             y_i[int(data[i, -1])] = 1\n",
    "\n",
    "#             # Forward pass\n",
    "#             activations = feed_forward(x_i, network)\n",
    "\n",
    "#             # Backpropagation\n",
    "#             deltas = [activations[-1] - y_i]\n",
    "#             for l in range(len(network) - 1, 0, -1):\n",
    "#                 delta = relu_derivative(np.dot(network[l]['W'], deltas[0]))\n",
    "#                 deltas.insert(0, delta)\n",
    "\n",
    "#             # Gradient descent parameter update\n",
    "#             for l, layer in enumerate(network):\n",
    "#                 layer['W'] -= learning_rate * np.outer(activations[l], deltas[l])\n",
    "#                 layer['b'] -= learning_rate * deltas[l]\n",
    "\n",
    "#     return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_size=0, hidden_layer_sizes=0, output_size=0):\n",
    "        super(DeepMLP, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_layer_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_layer_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_layer_sizes[-1], output_size))\n",
    "\n",
    "        # Combine all layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    # Feed Forward.\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalPipeline(StandalonePipeline):\n",
    "    def __init__(self, handler, trainer, test_loader, show_data=True):\n",
    "        super().__init__(handler, trainer)\n",
    "        self.show_data = show_data\n",
    "        self.test_loader = test_loader\n",
    "        self.loss, self.acc = [], []\n",
    "        self.ax = None\n",
    "        self.ax2 = None\n",
    "\n",
    "    def getLoss(self):\n",
    "        return self.loss\n",
    "    \n",
    "    def getAcc(self):\n",
    "        return self.acc\n",
    "\n",
    "    def main(self):\n",
    "        t = 0\n",
    "        while not self.handler.if_stop:\n",
    "            self.trainer.local_process(self.handler.downlink_package,\n",
    "                                       self.handler.sample_clients())\n",
    "            \n",
    "            for pack in self.trainer.uplink_package:\n",
    "                self.handler.load(pack)\n",
    "            \n",
    "            loss, acc = evaluate(self.handler.model, \n",
    "                                 nn.CrossEntropyLoss(),\n",
    "                                 self.test_loader)\n",
    "            if (self.show_data):\n",
    "                print(f\"Round {t}, Loss {round(loss,4)}, Test Acc {round(acc,4)}\")\n",
    "\n",
    "            self.loss.append(loss)\n",
    "            self.acc.append(acc)\n",
    "            \n",
    "            t += 1\n",
    "\n",
    "    def show(self):\n",
    "        plt.figure(figsize=(8,4.5))\n",
    "        self.ax = plt.subplot(1,2,1)\n",
    "        self.ax.plot(np.arange(len(self.loss)), self.loss)\n",
    "        self.ax.set_xlabel(\"Communication Round\")\n",
    "        self.ax.set_ylabel(\"Loss\")\n",
    "        \n",
    "        self.ax2 = plt.subplot(1,2,2)\n",
    "        self.ax2.plot(np.arange(len(self.acc)), self.acc)\n",
    "        self.ax2.set_xlabel(\"Communication Round\")\n",
    "        self.ax2.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like in the lab, we need to implement client-side training and server side aggregation.\n",
    "# I simply copy and pasted from my code:\n",
    "\n",
    "class NetworkOptions():\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.output_size = output_size\n",
    "\n",
    "def run(\n",
    "        training_data: np.ndarray,\n",
    "        test_data: np.ndarray,\n",
    "        input_size: int, \n",
    "        hidden_layer_sizes: int,\n",
    "        output_size: int,\n",
    "        epochs=1,\n",
    "        batch_size=1024,\n",
    "        eta=0.04,\n",
    "        cuda=True,\n",
    "        num_rounds=100,\n",
    "        num_clients=10\n",
    "    ):\n",
    "    # TODO: Get data lol\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    training_data = torch.tensor(training_data, dtype=torch.float32)\n",
    "    test_data = torch.tensor(test_data[:, -1], dtype=torch.long) \n",
    "\n",
    "    model = DeepMLP(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "    # Create an instance of the trainer for serial training on clients\n",
    "    trainer = SGDSerialClientTrainer(model=model,\n",
    "                                    num_clients=num_clients,\n",
    "                                    cuda=cuda)\n",
    "\n",
    "\n",
    "    # Once we actually HAVE the data, you can set it up as follows:\n",
    "    trainer.setup_dataset(training_data)\n",
    "\n",
    "    # Setup optimizer with the defined epochs, batch size, and learning rate\n",
    "    trainer.setup_optim(epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        lr=eta)\n",
    "\n",
    "    handler = SyncServerHandler(model=model, \n",
    "                                global_round=num_rounds,\n",
    "                                sample_ratio=0.1)\n",
    "\n",
    "    # TODO: I can't believe it, we STILL don't have data.\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "    standalone_eval = EvalPipeline(handler=handler, trainer=trainer, test_loader=test_loader)\n",
    "    standalone_eval.main()\n",
    "    standalone_eval.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
