{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from fedlab.contrib.algorithm.basic_client import SGDSerialClientTrainer\n",
    "from fedlab.contrib.algorithm.basic_server import SyncServerHandler\n",
    "from fedlab.core.standalone import StandalonePipeline\n",
    "from fedlab.utils.functional import evaluate\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible features to implement:\n",
    "\n",
    "- Transaction History\n",
    "    - Amt, Timestamps -> Frequency\n",
    "- Current Balance\n",
    "- User interaction\n",
    "- Geolocation Data\n",
    "- Time patterns\n",
    "- How often users redeem rewards\n",
    "- Wallet features used\n",
    "- Financial Goals\n",
    "\n",
    "-Some attributes are categorical like user interaction. We might have to do some sort of engagement leveling for that.\n",
    "- Wallet features can indicate what kind of resources a user might desire:\n",
    "    - If they like to check their balance more than making transactions, it might be a sign that a user is considering making a purchase but is nervous about consequences regarding it. This could be \"scenario 1\" and can be encoded as a one hot vector like [1, 0, 0, ..., 0]\n",
    "\n",
    "### NOTE\n",
    "\n",
    "- Most features are tentative and may not be implemented. It is unclear as to what kind of data we will have access to at the current moment and whether or not hte collection of this data is feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data that I think should be collected:\n",
    "- X[:, 0] = Amt (transaction amount/price)\n",
    "- X[:, 1] = Timestamps (block timestamp)\n",
    "- X[:, 2] = User's current balance.\n",
    "- X[:, 3] = Age\n",
    "- X[:, 4] = Number of total transactions made\n",
    "- X[:, 5] = Knowledge index based off of tests and such\n",
    "\n",
    "Output:\n",
    "- If 0, then they're doing ok\n",
    "- If 1, then they may be a little reckless\n",
    "- If 2, then they may be very reckless\n",
    "- If 3, 100% reckless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Prototype read data function.\n",
    "df = pd.read_csv(\"user-data.csv\", delimiter=\",\")\n",
    "df_out = pd.read_csv(\"not-normalized.csv\", delimiter=\",\")\n",
    "\n",
    "# Data matrix\n",
    "D = df.to_numpy()\n",
    "Y = df_out.to_numpy()[:, -1]\n",
    "\n",
    "# Presumably, (# of points, 5)\n",
    "print(D.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "training_data, testing_data = np.column_stack((D[:80, :], Y[:80])), np.column_stack((D[80:, :], Y[80:]))\n",
    "\n",
    "training_data = training_data.astype(np.float64)\n",
    "testing_data = testing_data.astype(np.float64)  # or np.int32 depending on your requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is code heavily based on Zaki's implementation of a Simple Neural Network.\n",
    "\n",
    "# def relu(z):\n",
    "#     \"\"\"Apply the ReLU (Rectified Linear Unit) function.\"\"\"\n",
    "#     return np.maximum(0, z)\n",
    "\n",
    "# def relu_derivative(z):\n",
    "#     \"\"\"Compute the derivative of the ReLU function.\"\"\"\n",
    "#     return np.where(z > 0, 1, 0)\n",
    "\n",
    "# def feed_forward(x, network):\n",
    "#     \"\"\"Perform a feedforward pass through the neural network.\"\"\"\n",
    "#     activations = [x]\n",
    "#     input_to_layer = x\n",
    "\n",
    "#     for layer in network:\n",
    "#         z = layer['b'] + np.dot(layer['W'].T, input_to_layer)\n",
    "#         input_to_layer = relu(z)\n",
    "#         activations.append(input_to_layer)\n",
    "\n",
    "#     activations[-1] = softmax(activations[-1])\n",
    "#     return activations\n",
    "\n",
    "# def initialize_network(input_size, hidden_layer_sizes, output_size, scale):\n",
    "#     \"\"\"Initialize a deep multilayer perceptron with random weights and biases.\"\"\"\n",
    "#     layer_sizes = [input_size] + hidden_layer_sizes + [output_size]\n",
    "#     network = []\n",
    "\n",
    "#     for i in range(len(layer_sizes) - 1):\n",
    "#         layer = {\n",
    "#             'b': np.random.rand(layer_sizes[i + 1]) * scale,\n",
    "#             'W': np.random.rand(layer_sizes[i], layer_sizes[i + 1]) * scale\n",
    "#         }\n",
    "#         network.append(layer)\n",
    "\n",
    "#     return network\n",
    "\n",
    "# def deep_mlp_training(data, output_size, max_iter, learning_rate, hidden_layer_sizes, scale):\n",
    "#     \"\"\"Train a deep multilayer perceptron on the given dataset.\"\"\"\n",
    "#     num_samples, num_features = data.shape\n",
    "#     input_size = num_features - 1  # Last column is assumed to be the label\n",
    "#     network = initialize_network(input_size, hidden_layer_sizes, output_size, scale)\n",
    "\n",
    "#     for j in range(max_iter):\n",
    "#         indices = np.arange(num_samples)\n",
    "#         np.random.shuffle(indices)\n",
    "\n",
    "#         for i in indices:\n",
    "#             x_i = data[i, :-1]\n",
    "#             y_i = np.zeros(output_size)\n",
    "#             y_i[int(data[i, -1])] = 1\n",
    "\n",
    "#             # Forward pass\n",
    "#             activations = feed_forward(x_i, network)\n",
    "\n",
    "#             # Backpropagation\n",
    "#             deltas = [activations[-1] - y_i]\n",
    "#             for l in range(len(network) - 1, 0, -1):\n",
    "#                 delta = relu_derivative(np.dot(network[l]['W'], deltas[0]))\n",
    "#                 deltas.insert(0, delta)\n",
    "\n",
    "#             # Gradient descent parameter update\n",
    "#             for l, layer in enumerate(network):\n",
    "#                 layer['W'] -= learning_rate * np.outer(activations[l], deltas[l])\n",
    "#                 layer['b'] -= learning_rate * deltas[l]\n",
    "\n",
    "#     return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDSerialClientTrainerTensor(SGDSerialClientTrainer):\n",
    "    def local_process(self, model_parameters, client_data):\n",
    "        if isinstance(client_data, torch.utils.data.dataset.Subset):\n",
    "            data_loader = DataLoader(client_data, batch_size=self.batch_size, shuffle=False)\n",
    "            pack = self.train(model_parameters, data_loader)\n",
    "            self.cache.append(pack)\n",
    "        else:\n",
    "            print(f\"Invalid data format for client data, type {type(client_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_size=0, hidden_layer_sizes=0, output_size=0):\n",
    "        super(DeepMLP, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_layer_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_layer_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_layer_sizes[i], hidden_layer_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_layer_sizes[-1], output_size))\n",
    "\n",
    "        # Combine all layers\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    # Feed Forward.\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalPipeline(StandalonePipeline):\n",
    "    def __init__(self, handler, trainer, test_loader, client_data, show_data=True):\n",
    "        super().__init__(handler, trainer)\n",
    "        self.show_data = show_data\n",
    "        self.test_loader = test_loader\n",
    "        self.client_data = client_data\n",
    "        self.loss, self.acc = [], []\n",
    "        self.ax = None\n",
    "        self.ax2 = None\n",
    "\n",
    "    def getLoss(self):\n",
    "        return self.loss\n",
    "    \n",
    "    def getAcc(self):\n",
    "        return self.acc\n",
    "\n",
    "    def main(self):\n",
    "        t = 0\n",
    "        while not self.handler.if_stop:\n",
    "            model_parameters = self.handler.downlink_package[0]\n",
    "\n",
    "            for client_id in self.handler.sample_clients():\n",
    "                client_data = self.client_data[client_id]\n",
    "                self.trainer.local_process(model_parameters, client_data)\n",
    "            \n",
    "            for pack in self.trainer.uplink_package:\n",
    "                self.handler.load(pack)\n",
    "            \n",
    "            loss, acc = evaluate(self.handler.model, \n",
    "                                nn.CrossEntropyLoss(),\n",
    "                                self.test_loader)\n",
    "            if (self.show_data):\n",
    "                print(f\"Round {t}, Loss {round(loss,4)}, Test Acc {round(acc,4)}\")\n",
    "\n",
    "            self.loss.append(loss)\n",
    "            self.acc.append(acc)\n",
    "            \n",
    "            t += 1\n",
    "\n",
    "    def show(self):\n",
    "        plt.figure(figsize=(8,4.5))\n",
    "        self.ax = plt.subplot(1,2,1)\n",
    "        self.ax.plot(np.arange(len(self.loss)), self.loss)\n",
    "        self.ax.set_xlabel(\"Communication Round\")\n",
    "        self.ax.set_ylabel(\"Loss\")\n",
    "        \n",
    "        self.ax2 = plt.subplot(1,2,2)\n",
    "        self.ax2.plot(np.arange(len(self.acc)), self.acc)\n",
    "        self.ax2.set_xlabel(\"Communication Round\")\n",
    "        self.ax2.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like in the lab, we need to implement client-side training and server side aggregation.\n",
    "# I simply copy and pasted from my code:\n",
    "\n",
    "def run(\n",
    "    training_data: np.ndarray,\n",
    "    test_data: np.ndarray,\n",
    "    Y_train: np.ndarray,\n",
    "    Y_test: np.ndarray,\n",
    "    input_size: int, \n",
    "    hidden_layer_sizes: list,\n",
    "    output_size: int,\n",
    "    show_data=True,\n",
    "    epochs=1,\n",
    "    batch_size=16,\n",
    "    eta=0.04,\n",
    "    cuda=False,\n",
    "    num_rounds=100,\n",
    "    num_clients=8\n",
    "):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    training_data_tensor = torch.from_numpy(training_data).float()\n",
    "    test_data_tensor = torch.from_numpy(test_data).float()\n",
    "    output_train_tensor = torch.from_numpy(Y_train).long()\n",
    "    output_test_tensor = torch.from_numpy(Y_test).long()\n",
    "\n",
    "    model = DeepMLP(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "    # Create an instance of the trainer for serial training on clients\n",
    "    trainer = SGDSerialClientTrainerTensor(model=model,\n",
    "                                    num_clients=num_clients,\n",
    "                                    cuda=cuda\n",
    "                                    )\n",
    "\n",
    "\n",
    "    # Once we actually HAVE the data, you can set it up as follows:\n",
    "    trainer.setup_dataset(training_data_tensor)\n",
    "\n",
    "    # Setup optimizer with the defined epochs, batch size, and learning rate\n",
    "    trainer.setup_optim(epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        lr=eta)\n",
    "\n",
    "    handler = SyncServerHandler(model=model, \n",
    "                                global_round=num_rounds,\n",
    "                                sample_ratio=0.1)\n",
    "\n",
    "    train_dataset = TensorDataset(training_data_tensor, output_train_tensor)\n",
    "    test_dataset = TensorDataset(test_data_tensor, output_test_tensor)\n",
    "\n",
    "    client_data_size = len(training_data_tensor) // num_clients\n",
    "    client_data = random_split(train_dataset, [client_data_size] * num_clients)\n",
    "\n",
    "    test_loader = DataLoader( test_dataset, batch_size=batch_size)\n",
    "    standalone_eval = EvalPipeline(handler=handler, trainer=trainer, test_loader=test_loader, client_data=client_data, show_data=show_data)\n",
    "    standalone_eval.main()\n",
    "\n",
    "    return standalone_eval.acc[-1], model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(\n",
    "    training_data: np.ndarray, \n",
    "    input_size: int, \n",
    "    output_size: int,\n",
    "    neuron_ranges=[16, 32, 64], \n",
    "    learning_rate_ranges=[0.01, 0.001]\n",
    "):\n",
    "    hyperparameter_combinations = itertools.product(neuron_ranges, learning_rate_ranges)\n",
    "    best_performance = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    for neurons, lr in hyperparameter_combinations:\n",
    "        kf = KFold(n_splits=5)\n",
    "        fold_performances = []\n",
    "\n",
    "        for train_index, val_index in kf.split(training_data):\n",
    "            X_train_fold, X_valid_fold = training_data[train_index], training_data[val_index]\n",
    "\n",
    "            # Assuming 'run' function is adapted for training and returns a trained model and its performance\n",
    "            acc, _ = \\\n",
    "                run (\n",
    "                    training_data=X_train_fold[:,:-1], \n",
    "                    test_data=X_valid_fold[:,:-1],\n",
    "                    Y_train=X_train_fold[:,-1],\n",
    "                    Y_test=X_valid_fold[:,-1],\n",
    "                    input_size=input_size,\n",
    "                    hidden_layer_sizes=neuron_ranges,\n",
    "                    output_size=output_size,\n",
    "                    eta=lr,\n",
    "                    show_data=False\n",
    "                )\n",
    "\n",
    "            # Evaluate the trained model on the validation set\n",
    "            fold_performances.append(acc)\n",
    "\n",
    "        average_performance = np.mean(fold_performances)\n",
    "\n",
    "        if average_performance < best_performance:\n",
    "            best_performance = average_performance\n",
    "            best_params = (neurons, lr)\n",
    "\n",
    "    return best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 0.01)\n"
     ]
    }
   ],
   "source": [
    "best_params = cross_validation(\n",
    "                training_data=training_data, \n",
    "                input_size=5,\n",
    "                output_size=4\n",
    "            )\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "optimal_hidden_layer_sizes = [best_params[0], 32]\n",
    "optimal_lr = best_params[1] \n",
    "\n",
    "# Train the model\n",
    "best_accuracy, model = run(\n",
    "    training_data=training_data[:,:-1],\n",
    "    test_data=testing_data[:,:-1],\n",
    "    Y_train=training_data[:,-1],\n",
    "    Y_test=testing_data[:,-1],\n",
    "    input_size=5,\n",
    "    hidden_layer_sizes=optimal_hidden_layer_sizes,\n",
    "    output_size=4,\n",
    "    epochs=1,  # Set your optimal number of epochs\n",
    "    batch_size=16,\n",
    "    eta=optimal_lr,\n",
    "    cuda=False,\n",
    "    num_rounds=200,\n",
    "    num_clients=8,\n",
    "    show_data=False\n",
    ")\n",
    "\n",
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the model is named 'model' and is part of your 'run' function or returned by it\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(optimal_hidden_layer_sizes, input_size=5, output_size=4):\n",
    "    # Initialize the model\n",
    "    model = DeepMLP(input_size, optimal_hidden_layer_sizes, output_size)\n",
    "\n",
    "    # Load the saved model parameters\n",
    "    model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Predicting for some new data\n",
    "# new_data = torch.tensor([your_new_data_here], dtype=torch.float32)\n",
    "# with torch.no_grad():  # Disable gradient calculation for inference\n",
    "#     prediction = model(new_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
